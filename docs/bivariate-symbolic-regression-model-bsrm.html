<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2 Bivariate symbolic regression model (BSRM) | iRegression: An R Package for Regression Models with Interval Variables</title>
  <meta name="description" content="Escrever um prefácio">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2 Bivariate symbolic regression model (BSRM) | iRegression: An R Package for Regression Models with Interval Variables" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://prdm0.github.io/iRegression/" />
  
  <meta property="og:description" content="Escrever um prefácio" />
  <meta name="github-repo" content="prdm0/bookiRegression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Bivariate symbolic regression model (BSRM) | iRegression: An R Package for Regression Models with Interval Variables" />
  
  <meta name="twitter:description" content="Escrever um prefácio" />
  

<meta name="author" content="Eufrasio de A. Lima Neto and Pedro Rafael D. Marinho">


<meta name="date" content="2019-01-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="sec-introduction.html">
<link rel="next" href="package.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="sec-introduction.html"><a href="sec-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="bivariate-symbolic-regression-model-bsrm.html"><a href="bivariate-symbolic-regression-model-bsrm.html"><i class="fa fa-check"></i><b>2</b> Bivariate symbolic regression model (BSRM)</a><ul>
<li class="chapter" data-level="2.1" data-path="bivariate-symbolic-regression-model-bsrm.html"><a href="bivariate-symbolic-regression-model-bsrm.html#preliminaries"><i class="fa fa-check"></i><b>2.1</b> Preliminaries</a></li>
<li class="chapter" data-level="2.2" data-path="bivariate-symbolic-regression-model-bsrm.html"><a href="bivariate-symbolic-regression-model-bsrm.html#subsec:model"><i class="fa fa-check"></i><b>2.2</b> The model</a></li>
<li class="chapter" data-level="2.3" data-path="bivariate-symbolic-regression-model-bsrm.html"><a href="bivariate-symbolic-regression-model-bsrm.html#subsec:parest"><i class="fa fa-check"></i><b>2.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.4" data-path="bivariate-symbolic-regression-model-bsrm.html"><a href="bivariate-symbolic-regression-model-bsrm.html#goodness-of-fit"><i class="fa fa-check"></i><b>2.4</b> Goodness-of-fit measure</a></li>
<li class="chapter" data-level="2.5" data-path="bivariate-symbolic-regression-model-bsrm.html"><a href="bivariate-symbolic-regression-model-bsrm.html#measures"><i class="fa fa-check"></i><b>2.5</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="package.html"><a href="package.html"><i class="fa fa-check"></i><b>3</b> The package</a><ul>
<li class="chapter" data-level="3.1" data-path="package.html"><a href="package.html#function-bivar"><i class="fa fa-check"></i><b>3.1</b> Function bivar</a></li>
<li class="chapter" data-level="3.2" data-path="package.html"><a href="package.html#sec:crm"><i class="fa fa-check"></i><b>3.2</b> Function crm</a></li>
<li class="chapter" data-level="3.3" data-path="package.html"><a href="package.html#sec:ccrm"><i class="fa fa-check"></i><b>3.3</b> Function ccrm</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">iRegression: An R Package for Regression Models with Interval Variables</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bivariate-symbolic-regression-model-bsrm" class="section level1">
<h1><span class="header-section-number">2</span> Bivariate symbolic regression model (BSRM)</h1>
<div id="preliminaries" class="section level2">
<h2><span class="header-section-number">2.1</span> Preliminaries</h2>
<p>Let <span class="math inline">\(Y = \{y_1,\ldots,y_n\}\)</span> be a set of observations that represents a random sample of the interval-valued variable <span class="math inline">\(Y\)</span>. Each observation <span class="math inline">\(y_{i} = [y_{Li},y_{Ui}] \in Y\)</span> is defined as an interval <span class="math inline">\(y \in \Im = \{[y_{L},y_{U}] : y_{L}, y_{U} \in \Re, y_{L} \leq y_{U}\}\)</span> and represents the observed value of the interval variable <span class="math inline">\(Y\)</span>. Despite the loss of information, we consider an interval-valued variable <span class="math inline">\(Y\)</span> as a two-dimensional or a bivariate quantitative feature vector <span class="math inline">\(\mathbf y_{i}=(y_{1i},y_{2i})\)</span>, where the variables <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are one-dimensional random variables representing, for example, the lower and upper boundaries <span class="math inline">\((Y^{L},Y^{U})\)</span> or the midpoint and half-range <span class="math inline">\((Y^{m},Y^{r})\)</span> of the intervals or any other pair of interval features possible to be represented.</p>
<p>Consider that the joint density probability function of the bivariate quantitative
feature vector <span class="math inline">\(\mathbf y_{i}=(y_{1i},y_{2i})\)</span> belongs to the bivariate exponential family of
distributions  defined by</p>
<p><span class="math display" id="eq:random">\[\begin{equation}
    f(\mathbf y; \pmb \theta)=
    \mathrm{exp}\left[\phi^{-1}\{y_{1}\theta_{1} + y_{2}\theta_{2} -
    b(\theta_{1},\theta_{2},\rho)\}+ c(y_{1},y_{2},\rho,\phi)\right],
    \tag{2.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta\)</span>=
<span class="math inline">\((\theta_{1},\theta_{2})\)</span> is the vector of canonical parameters, <span class="math inline">\(\phi\)</span> is a common dispersion parameter
and <span class="math inline">\(\rho\)</span> is a constant correlation parameter between these two
random variables. We assume that the functions
<span class="math inline">\(b(\cdot,\cdot,\cdot)\)</span> and <span class="math inline">\(c(\cdot,\cdot, \cdot, \cdot)\)</span> are known.
The function <span class="math inline">\(b(\cdot,\cdot,\cdot)\)</span> is the cumulant generating
function of () and the mean and variance of the bivariate
random vector <span class="math inline">\(\mathbf Y=(Y_1, Y_2)\)</span> can be obtained from well-known
equations of natural exponential families and presents a direct relation with the GLM framework. The log-likelihood function for the <span class="math inline">\(i\)</span>th observation can be written as</p>
<p><span class="math display" id="eq:loglikelihood">\[\begin{equation}
  l_{i} = l_{i}(\pmb \theta,\phi,\rho) =
        \phi^{-1} \{y_{1i}\theta_{1} +                                                                           y_{2i}\theta_{2}-b(\theta_{1},\theta_{2},\rho)\}+c(y_{1i},y_{2i},\rho,\phi).
\tag{2.2}
\end{equation}\]</span></p>
<p>The use of the bivariate exponential family of distributions allows to extend the GLM framework for the case of interval-valued variables.</p>
</div>
<div id="subsec:model" class="section level2">
<h2><span class="header-section-number">2.2</span> The model</h2>
<p>Let <span class="math inline">\(E=\{e_1,\ldots,e_n\}\)</span> be a set of examples that are described by
<span class="math inline">\(p+1\)</span> interval-valued variables <span class="math inline">\(Y\)</span>, <span class="math inline">\(T_{1},\ldots,T_{p}\)</span>. The interval-valued
variable <span class="math inline">\(Y\)</span> is a dependent variable and it is related to a set of interval-valued
variables <span class="math inline">\(T_j\)</span> (<span class="math inline">\(j=1,2,\ldots,p\)</span>), known as independent variables. Each example
<span class="math inline">\(e_{i}\in E\)</span> (<span class="math inline">\(i=1,\ldots,n\)</span>) is denoted by an interval quantitative feature
vector <span class="math inline">\((\mathbf t_{i},y_{i})\)</span>, with <span class="math inline">\(\mathbf t_{i} = (t_{i1},\ldots,t_{ip})\)</span>,
where <span class="math inline">\(t_{ij}=[a_{ij},b_{ij}] \in \Im = \{[a,b]: a, b \in \Re, a \leq b\}\)</span> <span class="math inline">\((j=1,\ldots,p)\)</span>
and <span class="math inline">\(y_{i} = [y_{Li},y_{Ui}] \in \Im\)</span> are the observed values of <span class="math inline">\(T_{j}\)</span> and <span class="math inline">\(Y\)</span>,
respectively.</p>
<p>Now, let <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(X_{1j}\)</span> and <span class="math inline">\(Y_2\)</span>, <span class="math inline">\(X_{2j}\)</span> (<span class="math inline">\(j=1,2,\ldots,p\)</span>) be quantitative variables
that represent any pair of interval features from the interval-valued variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(T_{j}\)</span>, respectively. In case where those variables represent, respectively, the lower and upper boundaries of the interval variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(T_{j}\)</span>,
each example <span class="math inline">\(e_{i} \in E\)</span> (<span class="math inline">\(i=1,\ldots,n\)</span>) will be denoted by two vectors: <span class="math inline">\((\mathbf x^{L}_{i},y^{L}_{i})\)</span>
and <span class="math inline">\((\mathbf x^{U}_{i}, y^{U}_{i})\)</span>, with <span class="math inline">\(\mathbf x^{L}_{i} = (x^{L}_{i1}, \ldots, x^{L}_{ip})\)</span> and <span class="math inline">\(\mathbf x^{U}_{i} = (x^{U}_{i1},\ldots, x^{U}_{ip})\)</span>, where <span class="math inline">\(x^{L}_{ij} = a_{ij}\)</span>, <span class="math inline">\(x^{U}_{ij} = b_{ij}\)</span>, <span class="math inline">\(y^{L}_{i} = y_{Li}\)</span> and <span class="math inline">\(y^{U}_{i} = y_{Ui}\)</span>
are the observed values of the quantitative variables <span class="math inline">\(X^{L}_{j}\)</span>, <span class="math inline">\(X^{U}_{j}\)</span>, <span class="math inline">\(Y^{L}\)</span> and <span class="math inline">\(Y^{U}\)</span>, respectively. Likewise, for the case where those variables
represent, respectively, the midpoint and half-range of the interval variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(T_{j}\)</span>,
each example <span class="math inline">\(e_{i} \in E\)</span> (<span class="math inline">\(i=1,\ldots,n\)</span>) will be denoted by the vectors
<span class="math inline">\((\mathbf x^{m}_{i}, y^{m}_{i})\)</span> and <span class="math inline">\((\mathbf x^{r}_{i}, y^{r}_{i})\)</span>,
with <span class="math inline">\(\mathbf x^{m}_{i} = (x^{m}_{i1}, \ldots, x^{m}_{ip})\)</span> and <span class="math inline">\(\mathbf x^{r}_{i} = (x^{r}_{i1},\ldots, x^{r}_{ip})\)</span>, where <span class="math inline">\(x^{m}_{ij} = (a_{ij} + b_{ij})/2\)</span>, <span class="math inline">\(x^{r}_{ij}=(b_{ij}-a_{ij})/2\)</span>,
<span class="math inline">\(y^{m}_{i}=(y_{Li}+y_{Ui})/2\)</span> and <span class="math inline">\(y^{r}_{i} = (y_{Ui} - y_{Li})/2\)</span> are the observed values of
the variables <span class="math inline">\(X^{m}_{j}\)</span>, <span class="math inline">\(X^{r}_{j}\)</span>, <span class="math inline">\(Y^{m}\)</span> and <span class="math inline">\(Y^{r}\)</span>, respectively.</p>
<p>Following the GLM framework,  consider a  with probabilistic support
defined by two components (a random and a syste-ma-tic component) to model interval-valued data.
The random component considers the bivariate random vector <span class="math display">\[\textbf{Y} = \left[\begin{array}{c}
Y_{1}\\
Y_{2}\end{array}\right],\]</span> having the bivariate exponential family ().
In the systematic component, the explanatory variables <span class="math inline">\(X_{1j}\)</span> and <span class="math inline">\(X_{2j}\)</span> (<span class="math inline">\(j=1,2,\ldots,p\)</span>)
are responsible for the variability of <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>, respectively, and they are defined by</p>
<p><span class="math display" id="eq:systematic">\[\begin{equation}
\pmb \eta_{1} = g_{1}(\pmb \mu_{1})= \pmb X_{1} \pmb \beta_{1}\,\, \mbox{and}\,\, \pmb \eta_{2} = g_{2}(\pmb \mu_{2})= \pmb X_{2} \pmb \beta_{2},
\tag{2.3}
\end{equation}\]</span>
where <span class="math inline">\(\pmb X_{1}\)</span> and <span class="math inline">\(\pmb X_{2}\)</span> are known model matrices
formed by the observed values of the variables <span class="math inline">\(X_{1j}\)</span> and <span class="math inline">\(X_{2j}\)</span>,
respectively, <span class="math inline">\(\pmb \beta_{1}\)</span> and <span class="math inline">\(\pmb \beta_{2}\)</span>
are vectors of parameters to be estimated,
<span class="math inline">\(\pmb \eta_{1}\)</span> and <span class="math inline">\(\pmb \eta_{2}\)</span> are
the linear predictors, <span class="math inline">\(\pmb \mu_{1}\)</span> and <span class="math inline">\(\pmb \mu_{2}\)</span>
are the mean of the res-pon-se variables <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>, respectively, with
<span class="math inline">\(\pmb \eta_{l}\)</span>=<span class="math inline">\((\eta_{l_{1}},\ldots, \eta_{l_{n}})^\top,\)</span> <span class="math inline">\(\pmb \mu_{l}\)</span> = <span class="math inline">\((\mu_{l_{1}},\ldots, \mu_{l_{n}})^\top\)</span> and <span class="math inline">\(\pmb \beta_{l}\)</span> <span class="math inline">\(=(\beta_{l_{0}},\ldots,\beta_{l_{p}})^\top\)</span>, <span class="math inline">\(l = 1,2\)</span>. Here, <span class="math inline">\(g_{1}(\pmb \mu_{1}\)</span>) and <span class="math inline">\(g_{2}(\pmb \mu_{2}\)</span>)
are well-known link functions that connect the mean of the res-pon-se variables <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> with the explanatory variables <span class="math inline">\(X_{1j}\)</span> and <span class="math inline">\(X_{2j}\)</span> (<span class="math inline">\(j=1,\ldots,p\)</span>), respectively, being possible to choose different link functions.
A few functions available for the  are:
, among others.
However, some link functions have particular properties and can be preferred
in some situations. For example, if one considers the half-range of intervals in the
random component, the logarithmic link function will guarantee positiveness
for the predicted values of <span class="math inline">\(\hat{y}_{i}^{r}\)</span> (<span class="math inline">\(\hat{y}_{i}^{r} &gt;0\)</span>)
and this result implies that <span class="math inline">\(\hat{y}_{i}^{L} \leq \hat{y}_{i}^{U}\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span>.</p>
</div>
<div id="subsec:parest" class="section level2">
<h2><span class="header-section-number">2.3</span> Parameter estimation</h2>
<p>The maximum likelihood method will be used as a theoretical basis for parameter estimation in the <strong>BSRM</strong>. In order to maximize the log-likelihood, <span class="citation">(Lima Neto, Cordeiro, and De Carvalho <a href="#ref-LimaNetoetal2011">2011</a>)</span> first assumed that <span class="math inline">\(\rho\)</span> is
fixed and then obtained the likelihood equations for estimating <span class="math inline">\(\pmb \beta_{1}\)</span> and <span class="math inline">\(\pmb \beta_{2}\)</span>. Both vectors can be estimated without knowledge of <span class="math inline">\(\phi\)</span>. In principle, <span class="math inline">\(\phi\)</span> could also be estimated by maximum likelihood although there may be practical difficulties associated with this method for some bivariate distributions in <a href="bivariate-symbolic-regression-model-bsrm.html#eq:random">(2.1)</a>. Next, a simple approach will be given to estimate <span class="math inline">\(\phi\)</span> based on the model deviance.</p>
<p>An algorithm for estimating these vectors of linear parameters can be
developed from the scoring method. Differentiating the total log-likelihood
<a href="bivariate-symbolic-regression-model-bsrm.html#eq:loglikelihood">(2.2)</a> yields the score functions for <span class="math inline">\(\pmb \beta_{1}\)</span>
and <span class="math inline">\(\pmb \beta_{2}\)</span></p>
<p><span class="math display">\[U(\beta_{1j}) = \frac{\partial{l(\pmb\beta_{1}},\pmb \beta_{2})}{\partial{\beta_{1j}}} = \frac{1}{\phi} \sum_{i=1}^{n} (y_{1i} - b^{(1)}_{i})\frac{\partial{\theta_{1}}}{\partial{\beta_{1j}}} = 
\frac{1}{\phi} \sum_{i=1}^{n} (y_{1i} - \mu_{1i})\frac{1}{V^{(1)}_{i}g&#39;_{1}(\mu_{1i})}x_{1_{ij}}\]</span></p>
<p>and</p>
<p><span class="math display">\[ U(\beta_{2j}) = \frac{\partial{l(\pmb\beta_{1}, \pmb \beta_{2}})}{\partial{\beta_{2j}}} = \frac{1}{\phi} \sum_{i=1}^{n} (y_{2i} - b^{(2)}_{i})\frac{\partial{\theta_{2}}}{\partial{\beta_{2j}}} = \frac{1}{\phi} \sum_{i=1}^{n} (y_{2r} - \mu_{2i})\frac{1}{V^{(2)}_{i}g&#39;_{2}(\mu_{2i})}x_{2_{ij}}.\]</span></p>
<p>In matrix notation, the score functions are</p>
<p><span class="math display" id="eq:score">\[\begin{equation}
\pmb U(\beta_{1}) = (\pmb X_{1})^\top \pmb W_{1} \pmb z_{1}\,\,\,
\mathrm{and}\,\,\,\pmb U(\pmb \beta_{2})= (\pmb X_{2})^\top \pmb W_{2} \pmb z_{2},
\tag{2.4}
\end{equation}\]</span>
where <span class="math inline">\(\pmb W_{1}\)</span> and <span class="math inline">\(\pmb W_{2}\)</span> are diagonal weighted matrices with corresponding elements</p>
<p><span class="math display">\[w_{1i}=[V^{(1)}_{i}\mbox{}g&#39;_{1}(\mu_{1i})^{2}]^{-1}\,\,\,\mbox{  and  }\,\,\,
w_{2i}=[V^{(2)}_{i}\mbox{}g&#39;_{2}(\mu_{2i})^{2}]^{-1},\]</span></p>
<p>and <span class="math inline">\(\pmb z_{1}\)</span> and <span class="math inline">\(\pmb z_{2}\)</span> are modified dependent variables related to <span class="math inline">\(\pmb y_{1}\)</span> and
<span class="math inline">\(\pmb y_{2}\)</span> given by</p>
<p><span class="math display">\[\pmb z_{1} = \pmb G_{1}(\pmb y_{1} - \pmb \mu_{1})\,\,\,\mbox{  and  }\,\,\,
\pmb z_{2} = \mathbf G_{2}(\mathbf y_{2} - \pmb \mu_{2}),\]</span></p>
<p>where <span class="math inline">\(\pmb G_{1} = \mathrm{diag}\{g&#39;_{1}(\mu_{1_{1}}),\ldots,g&#39;_{1}(\mu_{1_{n}})\}\)</span> and <span class="math inline">\(\pmb G_{2} = \mathrm{diag}\{g&#39;_{2}(\mu_{2_{1}}),\ldots,g&#39;_{2}(\mu_{2_{n}})\}\)</span> are <span class="math inline">\(n \times n\)</span> diagonal matrices.</p>
<p>The expected value of the <em>sv</em>-th component of the information matrix for the vector of
parameters <span class="math inline">\(\pmb \beta_{1}\)</span> is</p>
<p><span class="math display">\[\kappa_{1(s,v)} = -E\left[\frac{\partial^{2}\beta_{1}}{\partial{\beta_{1s}}\partial{\beta_{1v}}}\right] = E\left[\frac{\partial{l(\pmb \beta_{1})}}{\partial{\beta_{1s}}}\frac{\partial{l(\pmb \beta_{1}})}{\partial{\beta_{1v}}}\right] = E[U(\beta_{1s})U(\beta_{1v})].\]</span>
In matrix notation, the information matrices for <span class="math inline">\(\beta_{1}\)</span> and
<span class="math inline">\(\beta_{2}\)</span> can be written as</p>
<p><span class="math display">\[\pmb K_{1}=\phi^{-1}\, (\pmb X_{1})^\top \pmb W_{1} \pmb X_{1}\,\,\,\mbox{and}\,\,\,
\pmb K_{2}=\phi^{-1}\, (\pmb X_{2})^\top \pmb W_{2} \pmb X_{2}.\]</span></p>
<p>From the information matrices and equations <a href="bivariate-symbolic-regression-model-bsrm.html#eq:score">(2.4)</a>, we can use the scoring method to obtain the conditional maximum likelihood estimates (MLEs) of <span class="math inline">\(\pmb \beta_{1}\)</span> and
<span class="math inline">\(\pmb \beta_{2}\)</span> for a given <span class="math inline">\(\rho\)</span>. We have</p>
<p><span class="math display" id="eq:leastsquares">\[\begin{eqnarray}
\pmb \beta^{(k+1)} = \pmb \beta^{(k)} +  (\pmb X^\top \pmb W^{(k)} \pmb X)^{-1}\pmb X^\top \pmb W^{(k)} \pmb z^{(k)},
\tag{2.5}
\end{eqnarray}\]</span></p>
<p>where</p>
<p><span class="math display">\[\pmb \beta^{(k+1)} = \left[\begin{array}{c} \pmb \beta_{1}^{(k+1)}\\ \pmb \beta_{2}^{(k+1)}\end{array} \right],
\mbox{  } \mathbf X = \left[\begin{array}{cc} \pmb X_{1} &amp; \pmb 0\\ \pmb 0 &amp; \pmb X_{2} \end{array}\right],\]</span></p>
<p><span class="math display">\[\mathbf W^{(k)} = \left[\begin{array}{cc}
\pmb W_{1}^{(k)} &amp; \textbf{0}\\
\textbf{0} &amp; \pmb W_{2}^{(k)}\end{array}\right]\,\,\mbox{and}\,\,\,
\pmb z^{(k)} = \left[\begin{array}{c}
\pmb z_{1}^{(k)}\\
\pmb z_{2}^{(k)}\end{array} \right].\]</span></p>
<p>Expression <a href="bivariate-symbolic-regression-model-bsrm.html#eq:leastsquares">(2.5)</a> has the same form of the estimating equations for the GLMs. In general terms, we regress the modified dependent variable <span class="math inline">\(\mathbf z^{(k)}\)</span> on the local model matrix <span class="math inline">\(\pmb X\)</span> by taking <span class="math inline">\(\pmb W^{(k)}\)</span> as a modified weighted matrix. At <span class="math inline">\(k = 1\)</span>, an initial appro-xi-ma-tion
<span class="math inline">\(\pmb \beta^{(1)}\)</span> could be used to evaluate <span class="math inline">\(\pmb W^{(1)}\)</span> and <span class="math inline">\(\pmb z^{(1)}\)</span> from which equations () yield the next estimate <span class="math inline">\(\pmb \beta^{(2)}\)</span>. Hence, we update <span class="math inline">\(\pmb W^{(2)}\)</span> and <span class="math inline">\(\pmb z^{(2)}\)</span>, and so the iterations continue until the convergence is achieved and then the conditional MLE <span class="math inline">\(\hat{\pmb \beta}\)</span>, is obtained. In general, the convergence speed is fast, but it strongly depends on the choice of the initial value <span class="math inline">\(\pmb \beta^{(1)}\)</span>.</p>
</div>
<div id="goodness-of-fit" class="section level2">
<h2><span class="header-section-number">2.4</span> Goodness-of-fit measure</h2>
<p>Conditioned on the parameter <span class="math inline">\(\rho\)</span>, the discrepancy of a <strong>BSRM</strong> is defined by <span class="citation">(Lima Neto, Cordeiro, and De Carvalho <a href="#ref-LimaNetoetal2011">2011</a>)</span> as twice the difference between the maximum log-likelihood achievable and that achieved for the model under investigation. The discrepancy is known as the deviance of the current model and has the form of a genuine GLM deviance, since it is a function of the data only and of the MLEs <span class="math inline">\(\hat{\mu}_{1i}\)</span>
and <span class="math inline">\(\hat{\mu}_{2i}\)</span>, for <span class="math inline">\(i=1,\ldots,n\)</span>, which are calculated from the data. Hence, the deviance conditioned on <span class="math inline">\(\rho\)</span>, say <span class="math inline">\(D(\rho)\)</span>, can be written as</p>
<p><span class="math display" id="eq:deviance">\[\begin{eqnarray}
D(\rho) &amp;=&amp; 2 \sum_{i=1}^{n}\{y_{1i}[q_{1}(y_{1i},\rho) - q_{1}(\hat{\mu}_{1i},\rho)] + y_{2i}[q_{2}(y_{2i},\rho) - q_{2}(\hat{\mu}_{2i},\rho)]\nonumber\\
&amp;+&amp; [b(q_{1}(\hat{\mu}_{1i},\rho),q_{2}(\hat{\mu}_{2i},\rho),\rho) -
b(q_{1}(y_{1i},\rho),q_{2}(y_{2i},\rho),\rho)]\}.
\tag{2.6}
\end{eqnarray}\]</span></p>
<p>The direct maximum likelihood estimation of the dispersion parameter <span class="math inline">\(\phi\)</span> is a more difficult problem than the estimation of <span class="math inline">\(\beta\)</span>, and that complexity depends entirely on the functional form of the function <span class="math inline">\(c(y_1,y_2,\rho,\phi)\)</span>. For some , the MLE of the
dispersion parameter could be very complicated, the deviance can be used to obtain a consistent estimate
of the dispersion parameter <span class="math inline">\(\phi\)</span> from the estimates <span class="math inline">\(\hat\beta\)</span><span class="math inline">\(_{1}\)</span> and <span class="math inline">\(\hat\beta\)</span><span class="math inline">\(_{2}\)</span> obtained from (), with dimensions <span class="math inline">\(p_1 \times 1\)</span> and <span class="math inline">\(p_2 \times 1\)</span>, respectively. The deviance can be approximated by a <span class="math inline">\(\chi_{\nu}^2\)</span> distribution with <span class="math inline">\(\nu=2n-(p_1+p_2)\)</span> degrees of freedom, which leads to a simple estimate of <span class="math inline">\(\phi\)</span></p>
<p><span class="math display" id="eq:phiestimate">\[\begin{eqnarray}
\tilde{\phi} = \frac{D(\rho)}{2n - (p_{1} + p_{2})},
\tag{2.7}
\end{eqnarray}\]</span>
based on the fact that the deviance can be approximated by a chi-squared distribution.</p>
<p>Substituting the estimates <span class="math inline">\(\hat{\pmb \beta}_{1}\)</span>, <span class="math inline">\(\hat{\pmb \beta}_{2}\)</span> and <span class="math inline">\(\tilde\phi\)</span> in
<a href="bivariate-symbolic-regression-model-bsrm.html#eq:loglikelihood">(2.2)</a> yields the profile log-likelihood for the parameter <span class="math inline">\(\rho\)</span></p>
<p><span class="math display" id="eq:profile">\[\begin{equation}
l_p(\rho)= \phi^{-1}\sum_{i=1}^{n} \{y_{1i}\hat\theta_{1} +
y_{2i}\hat\theta_{2}-b(\hat\theta_{1},\hat\theta_{2},\rho)\} + \sum_{i=1}^{n}
c(y_{1i},y_{2i},\rho,\tilde\phi).
\tag{2.8}
\end{equation}\]</span></p>
<p>In the next step, the procedure calculates the profile log-likelihood <span class="math inline">\(l_p(\rho)\)</span> in <a href="bivariate-symbolic-regression-model-bsrm.html#eq:profile">(2.8)</a> for a trial series of va-lues of <span class="math inline">\(\rho \in\)</span> <span class="math inline">\([-1,1]\)</span> and determines numerically the value of the estimate <span class="math inline">\(\hat\rho\)</span> that maximizes <span class="math inline">\(l_p(\rho)\)</span>. Once the estimate <span class="math inline">\(\hat\rho\)</span> is obtained, it can be substituted into the algorithm <a href="bivariate-symbolic-regression-model-bsrm.html#eq:leastsquares">(2.5)</a> to produce the new conditional estimate <span class="math inline">\(\hat{\pmb \beta}\)</span>, and then substituting in equation <a href="bivariate-symbolic-regression-model-bsrm.html#eq:phiestimate">(2.7)</a> to obtain a new estimate <span class="math inline">\(\tilde\phi\)</span>. The new values of <span class="math inline">\(\hat{\pmb \beta}\)</span>, and <span class="math inline">\(\tilde\phi\)</span> can update <span class="math inline">\(\hat\rho\)</span>, and so the iterations continue until convergence is observed. The joint iterative process for the parameter estimates of the <strong>BSRM</strong> is included in the <strong>iRegression</strong> package through the function <code>bivar</code>.</p>
</div>
<div id="measures" class="section level2">
<h2><span class="header-section-number">2.5</span> Residuals</h2>
<p><span class="citation">(Lima Neto, Cordeiro, and De Carvalho <a href="#ref-LimaNetoetal2011">2011</a>)</span> present some residuals expressions that are useful to make inference about the response distribution, identify outliers, verify the link function adequacy, among others aspects. The <strong>BSRM</strong> allows an unique residual definition for an interval-valued data. Usually, the residuals are defined separately for each boundary of the interval.</p>
<p>The projection matrix <strong>H</strong> takes the form</p>
<p><span class="math display" id="eq:21h">\[\begin{eqnarray}
\pmb H = \pmb W^{1/2} \pmb X (\pmb X^\top \pmb W \pmb X)^{-1} \pmb X^\top \pmb W^{1/2},
\tag{2.9}
\end{eqnarray}\]</span></p>
<p>that is equivalent to replace <span class="math inline">\(\pmb X\)</span> by <span class="math inline">\(\pmb W^{1/2} \pmb X\)</span> which effectively allows for the change in variance with the mean. Here,</p>
<p><span class="math display">\[\pmb H = \left[ \begin{array}{cc}
\pmb H_{1} &amp; \pmb 0 \\
\pmb 0 &amp; \pmb H_{2} \end{array} \right], \mbox{ } \pmb X =
\left[ \begin{array}{cc}
\pmb X_{1} &amp; \pmb 0 \\
\pmb 0 &amp; \pmb X_{2} \end{array} \right]\,\, \mbox{and} \,\,\pmb
W = \left[ \begin{array}{cc}
\pmb W_{1} &amp; \pmb 0 \\
\pmb 0 &amp; \pmb W_{2} \end{array}\right].\]</span></p>
<p>The well-known measure of leverage is given by the diagonal elements of the projection matrix, with
<span class="math inline">\(\sum_{i} h_{1_{ii}} = p_{1}\)</span> and <span class="math inline">\(\sum_{i} h_{2_{ii}} = p_{2}\)</span>. Hence, the interval-valued observations for the response variable <span class="math inline">\(Y\)</span> with high leverage are indicated by <span class="math inline">\(h_{i}\)</span> = (<span class="math inline">\(h_{1_{ii}}\)</span> + <span class="math inline">\(h_{2_{ii}}\)</span>)
greater than <span class="math inline">\(2(p_1+p_2)/n\)</span>. An index plot of each <span class="math inline">\(h_{i}\)</span> versus <span class="math inline">\(i\)</span> with this lower limit could be an useful informal tool for looking at leverage.</p>
<p>Some residual measures commonly used in the GLM theory can be easily extended to the <strong>BSRM</strong>. The residual related to the <span class="math inline">\(i\)</span>th vector of observations <span class="math inline">\((y_{1i}, y_{2i})\)</span> can be composed by two parts: the residual for the observation <span class="math inline">\(y_{1i}\)</span> and the residual for the observation <span class="math inline">\(y_{2i}\)</span>. The Pearson residual is given by</p>
<p><span class="math display" id="eq:pearson">\[\begin{eqnarray}
r^{P}_{1i} = \frac{y_{1i} -\hat{\mu}_{1i}}{\sqrt{\widehat{V}_{1i}}}\,\,\,\mbox{and}\,\,\,r^{P}_{2i}
=\frac{y_{2i}-\hat{\mu}_{2i}}{\sqrt{\widehat{V}_{2i}}}.
\tag{2.10}
\end{eqnarray}\]</span></p>
<p>The Studentized Pearson residual which has a constant variance when <span class="math inline">\(\phi \rightarrow 0\)</span> can be expressed as</p>
<p><span class="math display" id="eq:studentized">\[\begin{eqnarray}
r^{SP}_{1i}= \frac{y_{1i}-\hat{\mu}_{1i}}{\sqrt{V(\hat{\mu}_{1i})(1-\hat{h}_{1_{ii}})}}\,\,\,
\mbox{and}\,\,\,r^{SP}_{2i}=\frac{y_{2i}-\hat{\mu}_{2i}}{\sqrt{V(\hat{\mu}_{2i})(1-\hat{h}_{2_{ii}})}}.
\tag{2.11}
\end{eqnarray}\]</span></p>
<p>The deviance residual can be interpreted as a joint residual measure or a global residual measure for the vector of observations <span class="math inline">\((y_{1i}, y_{2i})\)</span>. The <em>i</em>th deviance residual is defined in terms of the square root of the contribution of the <em>i</em>th observation to the deviance <a href="bivariate-symbolic-regression-model-bsrm.html#eq:deviance">(2.6)</a>. Conditioned on <span class="math inline">\(\rho\)</span>, the deviance residual can be expressed as</p>
<p><span class="math display" id="eq:devianceresidual">\[\begin{eqnarray} 
r_{i}^{D} &amp;=&amp; \mbox{sign}[(y_{1i}-\hat{\mu}_{1i})+(y_{2i}-\hat{\mu}_{2i})] \sqrt{d_{i}},
\tag{2.12}
\end{eqnarray}\]</span></p>
<p>where</p>
<p><span class="math display" id="eq:deviance1">\[\begin{eqnarray} 
d_{i} &amp;=&amp; 2\{y_{1i}[q_{1}(y_{1i},\rho)-q_{1}(\hat{\mu}_{1i},\rho)]+ y_{2i}[q_{2}(y_{2i},\rho) - q_{2}(\hat{\mu}_{2i},\rho)] \nonumber \\
&amp;+&amp; b(q_{1}(\hat{\mu}_{1i},\rho),q_{2}(\hat{\mu}_{2i},\rho),\rho)-b(q_{1}(y_{1i},\rho),q_{2}(y_{2i},\rho),\rho)\}. \nonumber
\tag{2.13}
\end{eqnarray}\]</span></p>
<p>Based on these residuals measures, the classical model checking techniques considered in the GLM theory can be straightforwardly extended for the <strong>BSRM</strong>.</p>
<hr />

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-LimaNetoetal2011">
<p>Lima Neto, Eufrásio Andrade, Gauss M. Cordeiro, and Francisco Assis T. De Carvalho. 2011. “Bivariate Symbolic Regression Models for Interval-Valued Variables.” <em>Journal of Statistical Computation and Simulation</em> 81 (11): 1727–44. <a href="https://doi.org/10.1080/00949655.2010.500470">https://doi.org/10.1080/00949655.2010.500470</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="package.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/prdm0/bookiReg/edit/master/bivariate_symbolic_regression_model_BSRM.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
